{
  "magic": "E!vIA5L86J2I",
  "timestamp": "2024-10-15T01:24:25.913123+00:00",
  "repo": "open-mpic/draft-mpic",
  "labels": [
    {
      "name": "bug",
      "description": "Something isn't working",
      "color": "d73a4a"
    },
    {
      "name": "documentation",
      "description": "Improvements or additions to documentation",
      "color": "0075ca"
    },
    {
      "name": "duplicate",
      "description": "This issue or pull request already exists",
      "color": "cfd3d7"
    },
    {
      "name": "enhancement",
      "description": "New feature or request",
      "color": "a2eeef"
    },
    {
      "name": "good first issue",
      "description": "Good for newcomers",
      "color": "7057ff"
    },
    {
      "name": "help wanted",
      "description": "Extra attention is needed",
      "color": "008672"
    },
    {
      "name": "invalid",
      "description": "This doesn't seem right",
      "color": "e4e669"
    },
    {
      "name": "question",
      "description": "Further information is requested",
      "color": "d876e3"
    },
    {
      "name": "wontfix",
      "description": "This will not be worked on",
      "color": "ffffff"
    }
  ],
  "issues": [
    {
      "number": 1,
      "id": "I_kwDOMYks0s6TACJZ",
      "title": "Move to organisation",
      "url": "https://github.com/open-mpic/draft-mpic/issues/1",
      "state": "CLOSED",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Shall we move this repo to an organisation?",
      "createdAt": "2024-08-14T16:09:14Z",
      "updatedAt": "2024-08-26T11:44:03Z",
      "closedAt": "2024-08-26T11:44:03Z",
      "comments": [
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "I would say we probably should, but I don't have any recommendations myself apart from a community-oriented org like open-mpic or something major like pkic.",
          "createdAt": "2024-08-17T01:14:57Z",
          "updatedAt": "2024-08-17T01:14:57Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I would be happy to host this at the open-mpic org. We could give everyone accounts in the org. I am also fine moving to a different organization if it is preferable.",
          "createdAt": "2024-08-21T15:22:39Z",
          "updatedAt": "2024-08-21T15:22:39Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "I'm happy with moving this to open-mpic.",
          "createdAt": "2024-08-21T15:55:02Z",
          "updatedAt": "2024-08-21T15:55:02Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "To this end, I invited everyone on this repo as a member of open-mpic. My understanding is this should allow you to create repos. @bwesterb I believe you can go into settings and change ownership to move this into the open-mpic org after you accept the invitation.",
          "createdAt": "2024-08-21T16:07:10Z",
          "updatedAt": "2024-08-21T16:07:10Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Move is done, thanks!",
          "createdAt": "2024-08-26T11:44:03Z",
          "updatedAt": "2024-08-26T11:44:03Z"
        }
      ]
    },
    {
      "number": 2,
      "id": "I_kwDOMYks0s6TAY_-",
      "title": "Add CA authentication mechanism for the API endpoints",
      "url": "https://github.com/open-mpic/draft-mpic/issues/2",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Probably best to make optional and suggest `Authorization` header for token.",
      "createdAt": "2024-08-14T17:00:26Z",
      "updatedAt": "2024-08-29T21:30:12Z",
      "closedAt": null,
      "comments": [
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I do like the Authorization header idea. I can't tell if I am speaking from too much of an AWS-specific standpoint but we will probably continue to use x-api-key header for open-mpic as this has builtin support from AWS API gateway. Authenticating with another header means we need to pass the request to our lambda function to perform the authentication which will incur a compute time bill for unauthenticated requests.\r\n\r\nRegardless of which header is used, I think having a simple HTTP header carry an authentication token is a good solution.",
          "createdAt": "2024-08-21T15:26:07Z",
          "updatedAt": "2024-08-21T15:26:07Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "What exactly are we planning to achieve with optional client authentication? If we dont plan to make it a requirement then its best to leave it out of the standard. Individual deployments can carry out their own authentication, as needed.\r\n\r\nWe can mention a recommendation in considerations though using `Authorization` request header.",
          "createdAt": "2024-08-28T23:38:25Z",
          "updatedAt": "2024-08-28T23:38:25Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "One goal is to make it easy for CAs to switch between MPIC providers. If that is the case, we'd like the authorization methods not to diverge too much.",
          "createdAt": "2024-08-29T10:53:46Z",
          "updatedAt": "2024-08-29T10:53:46Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "We can keep it optional as recommendation imo.",
          "createdAt": "2024-08-29T21:30:11Z",
          "updatedAt": "2024-08-29T21:30:11Z"
        }
      ]
    },
    {
      "number": 3,
      "id": "I_kwDOMYks0s6TAZp3",
      "title": "Make quorum deterministic?",
      "url": "https://github.com/open-mpic/draft-mpic/issues/3",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "We do not want retries to help the attacker.",
      "createdAt": "2024-08-14T17:01:58Z",
      "updatedAt": "2024-08-30T02:45:33Z",
      "closedAt": null,
      "comments": [
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "There's a retry logic proposal I have written up on the open-mpic-specification here: https://github.com/open-mpic/open-mpic-specification/issues/3\r\nAgreed that some level of determinism is good so that retries _can_ (to a degree) be useful for the issuer but not for an adversary.",
          "createdAt": "2024-08-17T01:18:43Z",
          "updatedAt": "2024-08-17T01:18:43Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I agree with @sciros In open MPIC we are going with a `max-attempts` field to govern retries with the following description language:\r\n\r\n\r\n> The maximum number of times validation or CAA checks should be retried as the result of a single API POST request. An implementation may choose to retry with the same or different perspectives each time. It is recommended implementations cap the number of distinct sets of perspectives that will ever validate a particular identifier to avoid adversaries retrying many times in the interest of getting favorable perspective sets.",
          "createdAt": "2024-08-21T15:29:03Z",
          "updatedAt": "2024-08-23T12:27:46Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "I believe details about max number of retries is an implementation specific characteristic, not something that the standard should define.\r\n\r\nI am not sure if the client will have the right knowledge or confidence to determine if they should go with the same set of perspective or different on retries when given the option. I am leaning towards keeping it deterministic (the client should not have to deal with it) to prevent an adversary creating a false quorum, while maintaining simplicity in design. \r\n\r\nA retry by nature means re-doing the _same_ work. I suspect some CAs in the future might use multiple MPIC services as fallback on failures.",
          "createdAt": "2024-08-29T00:28:05Z",
          "updatedAt": "2024-08-29T01:27:57Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Perhaps this is a separate issue, but I'd like to discuss why the MPIC service should be the one to retry. An alternative is for the CA to retry the request to the MPIC service.\r\n\r\n@SulemanAhmadd wrote\r\n\r\n> I believe details about max number of retries is an implementation specific characteristic, not something that the standard should define.\r\n\r\nIt depends how the retry is implemented. If the vantage points change on each try, then it degrades security. If it's deterministic as you assume, then it's less problematic.",
          "createdAt": "2024-08-29T10:57:14Z",
          "updatedAt": "2024-08-29T11:01:21Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "It's a question of balancing the value of retries vs not arriving at a \"false quorum\" as @SulemanAhmadd mentioned.\r\n\r\nIf each retry uses the same perspectives every time then whether retries are done by the client or by the service makes no difference. If there is actual logic to making retries more useful than that, then I believe it belongs in the service and should be opaque to the client.\r\n\r\nThe specific retry logic I proposed is discussed here: https://github.com/open-mpic/open-mpic-specification/issues/3\r\nIt attempts to maintain determinism while providing some value to retrying corroboration with disjoint sets of perspectives (cohorts). The client does not get to pick what perspectives are in what disjoint set, and there is a small and finite number of such cohorts that would be created.\r\n\r\nThis is another situation where the two paradigms -- sellf-hosted vs SaaS -- carry different implications about what makes the most sense to do.\r\n\r\nI can't assume that a self-hosted perspective will always be operating perfectly. As a result, there can be value to retries for a self-hosted solution, as they could provide feedback on the behavior of individual perspectives while not letting perspectives that are failing for reasons other than BGP Hijacking mislead corroboration (e.g., London AWS Lambda is down for whatever reason), yet still ensure that passing corroboration follows the spirit and letter of the requirements. ",
          "createdAt": "2024-08-29T16:12:55Z",
          "updatedAt": "2024-08-30T02:45:33Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "> Perhaps this is a separate issue, but I'd like to discuss why the MPIC service should be the one to retry. An alternative is for the CA to retry the request to the MPIC service.\r\n\r\nMy comment assumed we were discussing deterministic retries initiated _by the client_ (CA) in this thread. Client retries and MPIC service internal retries are two separate issues imo.\r\n\r\nSo for client initiated retries, I think it makes sense to be deterministic on the set of perspectives each time. We dont have to define an upper limit on possible retries in the standard. Need to make sure adversaries cannot trick the MPIC service to choose its favorable perspectives.\r\n\r\nFor internal MPIC service retries, I feel @sciros approach is a good one. The service can have the ability to choose different 'healthy' perspectives if some run fails with 'non-healthy' perspectives outcomes (where health is determine by some error types that we can define). This keeps choice of perspectives completely opaque and deterministic to the client at least, while still maintaining useful internal retry logic. Keeping it opaque is the key.\r\n\r\nThoughts?",
          "createdAt": "2024-08-29T19:25:13Z",
          "updatedAt": "2024-08-29T19:25:13Z"
        }
      ]
    },
    {
      "number": 4,
      "id": "I_kwDOMYks0s6TAZ7h",
      "title": "Cyclic dependency in trust",
      "url": "https://github.com/open-mpic/draft-mpic/issues/4",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Should the CA trust the WebPKI to authenticate the MPIC service?",
      "createdAt": "2024-08-14T17:02:41Z",
      "updatedAt": "2024-09-06T12:01:46Z",
      "closedAt": null,
      "comments": [
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I know this is ugly, but in the end of ensuring a clean implementation, I think a standard HTTPS connection simplifies things a lot. We could use some SHOULD language to say something like: \r\n\r\nTo avoid dependence on the webPKI, CAs deploying MPIC APIs SHOULD authenticate MPIC API endpoints using self-signed certificates or private trust anchors. MPIC API endpoints MUST be authenticated and traffic to an MPIC API endpoint MUST NOT be sent in plaintext.\r\n\r\nIMO ensuring endpoint authentication is more important than the cyclic trust issue.",
          "createdAt": "2024-08-21T15:32:58Z",
          "updatedAt": "2024-08-21T15:32:58Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "If we let the MPIC service sign it responses, do we still need a certificate for the MPIC endpoint? I wonder if CT logs also follow the same signed responses trust model. Only using signed responses though means we are comfortable with plaintext requests.\r\n\r\nAlso, there is a notion of whether the same authentication requirements apply for CA -> MPIC service and internally for MPIC service for contacting different perspectives.",
          "createdAt": "2024-08-29T21:19:24Z",
          "updatedAt": "2024-08-29T21:19:24Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "If the attacker can manipulate the communication between CA and MPIC service, then signing the response is not enough. A simple example is the following. The attacker might change the domain name of the request to an attacker controlled domain name. If the response doesn't contain the domain name, then the CA might be convinced of domain control where none is present.",
          "createdAt": "2024-09-02T14:55:50Z",
          "updatedAt": "2024-09-02T14:55:50Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "Right, but if the MPIC service signs over the request + response attributes then it can provide integrity of what was received and the outcome of MPIC process, to the client. Would that be sufficient?",
          "createdAt": "2024-09-03T17:35:38Z",
          "updatedAt": "2024-09-03T17:35:38Z"
        },
        {
          "author": "gcimaszewski",
          "authorAssociation": "COLLABORATOR",
          "body": "From the docs, it looks like Cloudflare's Multipath DCV API does not return the exact URL/domain name paths that were validated (although it does gives the details of the CAA records). For a Cloudflare-like spec without authentication on the requests from CA to MPIC, the attack that @bwesterb described is likely feasible. \r\nThe current Open MPIC spec does return the full parameters used for the challenge queries in the `validation-details` field within the `ValidationResponse` type. For this case, a signature on the response will allow the CA to check that the MPIC service received the correct validation parameters. I agree with @birgelee that simply designating HTTPS would take care of both problems. AWS only allows HTTPS endpoints for its API services anyway.",
          "createdAt": "2024-09-03T18:10:33Z",
          "updatedAt": "2024-09-03T18:10:33Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "My high level thought is whether or not we have the signing service, the API must be run over HTTPS. I think this should be specified in the RFC.\r\n\r\nI also agree that @bwesterb 's attack is viable if the API does not echo back the request in its response. If it does, I think this attack is mitigated.\r\n\r\nI would put in a recommendation for echoing API call parameters in the response as not only does it mitigate this attack, but it makes a self-contained log object that does not need to be cross joined with the original requests. I know this sounds somewhat silly, but having done a good amount of analysis on Let's Encrypt logs, log lines that only make sense in context (i.e., response 1234 needs to be analyzed with request 1234 to know what domain was being checked) are a huge hassle in post analysis. I feel from an audit standpoint and an implementation simplicity standpoint, just echoing parameters improves the usability of the API.",
          "createdAt": "2024-09-03T19:29:16Z",
          "updatedAt": "2024-09-03T19:29:16Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Echoing will work for this particular attack if the client also checks that the request in the response matches the origin request. These are the kind of things that clients tend to forget.\r\n\r\n> My high level thought is whether or not we have the signing service, the API must be run over HTTPS. I think this should be specified in the RFC.\r\n\r\nI agree. Let's lean as much on TLS as possible for transport security and not try to reinvent or complicate things too much.\r\n\r\nHaving an audit trail is a more convincing argument for echoing.",
          "createdAt": "2024-09-03T19:54:57Z",
          "updatedAt": "2024-09-03T19:54:57Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "I am not sure how self-signed certs will provide us meaningful value on top of signed service responses (based on request+response data). Clients (CA) would have to verify the signature anyways. Confidentiality is not a concern given current requirements. We should consider ease of service deployment / management alongside. Lets aim to get more feedback on this at IETF.\r\n\r\nOn logging, though a separate issue, I agree on that. That one can be a MUST.",
          "createdAt": "2024-09-03T20:27:45Z",
          "updatedAt": "2024-09-03T20:27:45Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "@SulemanAhmadd see my comment in #6 on why TLS alone is likely not sufficient. ",
          "createdAt": "2024-09-06T12:01:44Z",
          "updatedAt": "2024-09-06T12:01:44Z"
        }
      ]
    },
    {
      "number": 5,
      "id": "I_kwDOMYks0s6TAaNj",
      "title": "Validations methods besides http, dns, and caa",
      "url": "https://github.com/open-mpic/draft-mpic/issues/5",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Do we want to add alpn?",
      "createdAt": "2024-08-14T17:03:27Z",
      "updatedAt": "2024-08-19T20:45:33Z",
      "closedAt": null,
      "comments": [
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "Right now the open-mpic API is specified to support http, dns, alpn, and caa.\r\nIs there a reason to hold off on supporting that with v1? If not then we can forge ahead with it.",
          "createdAt": "2024-08-17T02:42:25Z",
          "updatedAt": "2024-08-17T02:42:25Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "alpn is more difficult to implement, and it's rarely used. We could leave it out (for now), make it optional to implement (for now), or insist on it.",
          "createdAt": "2024-08-19T09:27:33Z",
          "updatedAt": "2024-08-19T09:27:33Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "I would vote for limiting the scope for the first draft. ALPN can be a future extension.",
          "createdAt": "2024-08-19T19:47:47Z",
          "updatedAt": "2024-08-19T19:47:47Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "That works for me. It's possible that real-world implementations will end up functionally ahead of the IETF draft given the current requirement timelines, so we would then just maintain a tagged reference implementation that conforms precisely to the draft spec (no ALPN).",
          "createdAt": "2024-08-19T20:45:31Z",
          "updatedAt": "2024-08-19T20:45:31Z"
        }
      ]
    },
    {
      "number": 6,
      "id": "I_kwDOMYks0s6TAaYE",
      "title": "Sign responses",
      "url": "https://github.com/open-mpic/draft-mpic/issues/6",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Do we want the MPIC service to sign its responses as evidence.",
      "createdAt": "2024-08-14T17:03:51Z",
      "updatedAt": "2024-09-06T16:13:26Z",
      "closedAt": null,
      "comments": [
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "Depends on the anticipated client-service relationship(s) it'll support. At the moment I believe the open-mpic spec and implementation effectively assumes that it's to be deployed and maintained by the same organization that is acting as the client (i.e. a CA is using it for its own MPIC), then its effectively a service within that org and signing the responses doesn't add much in the way of trust (I think), especially if there are service logs available and accessible by the organization.\r\n\r\nIf we want the API to be crafted to support a client-service relationship where the API is basically externally facing (and I suppose that's the most practical reason to have a \"standard\" API at all, for there to be multiple externally available MPIC APIs hosted by, e.g., Cloudflare, etc.) then I think it's a valid question as to whether traceability and non-repudiation have a place in this. Perhaps there can be a kind of \"mode\" (configuration) under which the API is expected to be deployed -- externally facing, in which case a whole set of additional specification applies, or internally facing, in which case things are slightly simpler. Just thinking out loud.",
          "createdAt": "2024-08-17T03:01:32Z",
          "updatedAt": "2024-08-17T03:01:32Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I think signed responses might be helpful particularly if it is third party run. This could remove any suspicion on the part of the CA and immediately show that the CA called the API. I like moving more towards CA accountability and transparency and if a CA saves its API responses with signatures, we no longer need to trust the CAs word that they did MPIC, there is now an unquestionable log.",
          "createdAt": "2024-08-21T15:36:32Z",
          "updatedAt": "2024-08-21T15:36:32Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree with the upsides. There is also a clear downside: signing responses requires a key that needs to be kept secure, and might be rolled once in a while.",
          "createdAt": "2024-08-23T12:24:07Z",
          "updatedAt": "2024-08-23T12:24:07Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "Would it be out of the question to make signed responses optional? I personally think it makes a lot more sense in the CloudFlare case than Open MPIC. In Open MPIC CAs operate their own APIs so they would have access to the signing key, significantly reducing the value of signed responses.",
          "createdAt": "2024-08-25T04:44:17Z",
          "updatedAt": "2024-08-25T04:44:17Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "I personally believe that this should be optional. It has value for SaaS API deployments. It doesn't really for self-hosted deployments. I feel like open-mpic implementation would not implement that part of the API since it's not trying to enable a SaaS model.",
          "createdAt": "2024-08-25T07:06:20Z",
          "updatedAt": "2024-08-25T07:06:20Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Optional sounds good. I'd like to suggest to punt this to after the initial draft.\r\n\r\n(There is of course a relation with #4: if in practice OpenMPIC users are using self-signed certificates or pinning, then we could do this instead, as it's essentially shouldering the same downsides.)",
          "createdAt": "2024-08-26T11:19:03Z",
          "updatedAt": "2024-08-29T11:18:03Z"
        },
        {
          "author": "gcimaszewski",
          "authorAssociation": "COLLABORATOR",
          "body": "Including the signing functionality will likely make the implementation even more cloud provider-specific - the signing keys would need to be stored and accessed through a key management or secrets storage service, and AWS alone offers several of them. \r\nThis would probably also need more discussion for how the signing key is decided on, administered, and rotated (especially if the signatures are intended to serve as a transparency log). \r\nI would vote on omitting it from an MPIC draft, but I do think the idea of creating a verifiable log that MPIC was performed is super interesting. ",
          "createdAt": "2024-08-29T11:11:31Z",
          "updatedAt": "2024-08-29T11:11:31Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "I think signed responses will be great for transparency and accountability. CAs can use the signatures to validate that they used a specific MPIC operator. \r\n\r\n@gcimaszewski comments do resonate though. There is an additional consideration on how to discover the MPIC public keys. We can keep it out of the first iteration as a requirement but recommend in the considerations.",
          "createdAt": "2024-08-29T21:26:16Z",
          "updatedAt": "2024-08-29T21:26:16Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "[speaking with my Max-Planck Institute for Informatics hat on here]\r\n\r\nI think signing responses with this is an absolute must. TLS provides message integrity and confidentiality guarantees, but does not provide long term non-repudiation. A general goal of WebPKI infrastructure is to be able to detect (and possibly blame) mis-behaving parties. \r\n\r\nOn the specifics how to achieve this I'm less certain. RFC 9421 HTTP Signatures seems like exactly what would be needed, and already exists within the IETF ecosystem. For this protocol we could define the `keyid` to be a URL to an X.509 with a new mpicServerCertification EKU. The SAN of that certificate can then bind the MPIC response to a domain as the MPIC provider's identity. \r\n\r\nIf we go down this route with X.509 certs then a mpicClientAuthorization EKU could also be used to solve #2.\r\n\r\nI'm open to suggestions for better ways to do this, but I'm not willing to accept that non-refutable signatures aren't a core part of this protocol. ",
          "createdAt": "2024-09-06T11:41:25Z",
          "updatedAt": "2024-09-06T11:41:25Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "> I'm open to suggestions for better ways to do this, but I'm not willing to accept that non-refutable signatures aren't a core part of this protocol.\r\n\r\nWith ACME, the subscriber doesn't sign the challenge with a non-refutable signature.\r\n\r\nI think it's good to reason in concrete attacks here. When would the signature be useful? It doesn't help in the obvious case of a malicious MPIC service: that malicious service can simply claim it saw certain HTTP responses and then sign it. There is nothing to compare those claims to. I am not yet convinced that signing is categorically necessary here.\r\n\r\nI don't think RFC 9421 signatures are ideal: they're very much tied to the HTTP context. We want CAs to extract them. I think it makes sense to piggy-back on the simple signature method used in CT RFC6962 with which CAs are already familiar.\r\n",
          "createdAt": "2024-09-06T12:16:58Z",
          "updatedAt": "2024-09-06T12:16:58Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "> With ACME, the subscriber doesn't sign the challenge with a non-refutable signature.\r\n\r\nYou were not to know this, but I have had some discussions with the Chrome recently about long term verifiable DCV. The basic idea is to log the validation used by the CA publicly (a la CT) and require the website to keep that available for the duration of the certificate.\r\n\r\nSignatures over MPIC would form part of this imo. ",
          "createdAt": "2024-09-06T14:35:32Z",
          "updatedAt": "2024-09-06T14:35:32Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "So I think signatures do help in the case where a CA misbehaves and is not calling MPIC but wants to pretend they are. Essentially, should a CA wish to generate fraudulent validation logs claiming they did validation when they did not, MPIC service signatures from a 3rd party help.\r\n\r\nI personally still think they should be optional. If a CA runs the MPIC service, I don't really think they are helpful. If these were to be implemented in Open MPIC, I would probably just have CAs input the signing keys as some sort of config file as opposed to using any type of AWS key management service (to avoid additional AWS dependency) and there would be no security against the CA owner taking the key and signing arbitrary responses.\r\n\r\nFinally, I would advise against any signature scheme that expects HTTP messages. The core API object has been put mostly into JSON and I would not be surprised if the transport of these JSON requests/responses eventually moved off HTTPS in some implementations (e.g., Apache Kafka events, gRPC calls, etc...).",
          "createdAt": "2024-09-06T16:13:25Z",
          "updatedAt": "2024-09-06T16:13:25Z"
        }
      ]
    },
    {
      "number": 7,
      "id": "I_kwDOMYks0s6TAbNg",
      "title": "How much levers to give the CA in the strictness of validation",
      "url": "https://github.com/open-mpic/draft-mpic/issues/7",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Should we allow the client to specify the details of the quorum? Eg. min 2 out of 10. Or just a policy? Eg. CA/B 2026. Or leave it completely to the service.",
      "createdAt": "2024-08-14T17:05:28Z",
      "updatedAt": "2024-08-25T04:46:59Z",
      "closedAt": null,
      "comments": [
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "We were just having this conversation earlier today about how \"requirements-aware\" the API should be and to what extent it should enforce them.\r\nThe policy to specify is a really interesting idea.\r\nAs of this moment, the open-mpic API allows for specification of any value for the quorum, with the value of 0 effectively rendering it a logging-only service that interprets all outcomes as passing. I am thinking of proposing a change to that, and have a pull request that's under review which nudges validation logic more towards the requirements-aware side of things.\r\nAt the same time, the quorum count is currently an optional parameter. In its absence, the default can either be fully configurable by whoever owns the services (i.e. set it to something like 5 in a config file) or it can be programmatically derived based on the count of remote perspectives being interrogated (i.e. `perspective_count - 1` or `perspective_count - 2` depending on whether that count is <6 or not.)\r\n\r\nAnyway in the interest of making fast progress on this, here's my current thinking and what I'm considering implementing in the coming days, though I'm 100% OK with changing completely if my reasoning is faulty. \r\n\r\nI propose that quorum count remain an optional parameter to specify to the service, to allow for a higher-than-minimum-allowed-by-the-requirements threshold. If you want a quorum of 100%, that should be possible.\r\n\r\nI also propose that the default quorum is requirements-aware (programmatically derived). The logic used for deriving it can be policy-specific... that's something I hadn't thought of. I don't have a strong opinion on whether that should be how the service applies logic (i.e., it can apply a _non-current policy_) or whether it should just always be updated to reflect the current policy, and versioned accordingly. Perhaps the latter?\r\n\r\nI also propose that the quorum count in the API request (assuming it's a whole number) is allowed to be below the required minimum only if the API is run in a diagnostics/dev mode with value validation (as opposed to request structure validation) disabled. That, or include a warning in the response that the quorum count used is below the required threshold and therefore cannot actually be used to support cert issuance. Not sure if the latter is really a useful use case to support, though.",
          "createdAt": "2024-08-17T03:23:18Z",
          "updatedAt": "2024-08-17T03:23:18Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "There are two points to this conversation I would bring up.\r\n\r\n1. (which @sciros already mentioned) that quorum requirements change over time so a policy that is CAB Forum compliant now might not be in the future. Truly adhering to the CAB spec would mean changing the policy over time based on the implementation dates.\r\n2. This is more broad than just quorum, the number of perspectives used in the first place is also related to the CAB F requirements. If we fully take this route of the API enforcing the requirements, does it make sense to force the perspective count as well (i.e., asking for 2 perspectives after 3 are required throws an error).\r\n\r\nI am still thinking a bit more on this. I like auto quorum (so if you don't specify a quorum we go with what the CAB Forum says). I am still now sure how much the API should yell at you if you do something not CAB F compliant.",
          "createdAt": "2024-08-21T15:42:58Z",
          "updatedAt": "2024-08-21T15:42:58Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "What about two options:\r\n\r\n1. `balance` (default) Let the MPIC service choose. This allows the MPIC service to loosen or tighten requirements with more insight. Of course needs to meet current CA/B guidelines.\r\n2. `baseline` Follows the current minimum CA/B minimum requirements, and nothing more.\r\n\r\nI thought about adding a third `strict` option but I'm not quite sure how to define it.\r\n\r\n And of course we add an optional-to-implement API for the exact count.",
          "createdAt": "2024-08-23T12:21:09Z",
          "updatedAt": "2024-08-23T12:21:09Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I like the balance, baseline options as long as it coexists with a way to specify the exact count. I do like the idea of also having a strict option which I think should be defined are requiring a success from all perspectives used. Another name could be \"full\" for the option.",
          "createdAt": "2024-08-25T04:46:58Z",
          "updatedAt": "2024-08-25T04:46:58Z"
        }
      ]
    },
    {
      "number": 8,
      "id": "I_kwDOMYks0s6TYF9O",
      "title": "Endpoint naming",
      "url": "https://github.com/open-mpic/draft-mpic/issues/8",
      "state": "OPEN",
      "author": "sciros",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "This question has come up as part of open-mpic as well but we may as well discuss it all here to keep things synced up.\r\n\r\nWhat should the endpoints be for the various validations/checks that we'll want performed? Due to the nature of the industry terms, there's no obviously elegant approach.\r\n\r\nI figured an easy way to start the conversation would be to throw some ideas out there to beat up on. Gemini and ChatGPT were happy to oblige, though each gave me quite different results for my question about this. Here was my prompt:\r\n\r\n> Let's talk about PKI and certificate issuance. I want to brainstorm endpoint naming for an API that would do multi-perspective issuance corroboration for various validation checks.\r\nThere are several kinds of \"verifications\" or \"validations\" that get run as part of issuing a signed certificate. Correct me where I'm wrong, because I want to be very technically pedantic about this, but I think of the checks as being, roughly, DCV (domain control validation) and then CAA (certificate authority authorization) checking. With DCV the word \"validation\" is already in the name, and with CAA it's not, so I think many people in the PKI space do refer to CAA record validation as a \"CAA check,\" and I've seen the occasional article where, perhaps for the sake of parallel terminology, people will talk of a \"DCV check.\" But if I wanted to have an API with endpoints that carry out these various verifications, what is a good way to organize and name them? Perhaps DCV should be broken out into the various validation methods? And then CAA is just alongside those as \"yet another record type\" to check the presence of? Or maybe it's \"/dcv-check\" and \"/caa-check\" and then specifying in the request body what kind of DCV validation type to carry out (HTTP, etc.)? Please ruminate on this for me.\r\n\r\nGemini gave me the following options:\r\n\r\n> **Option 1:** Hierarchical Structure; clear separation of concerns, detailed endpoint names.\r\n> `/validations/dcv`\r\n> `/validations/dcv/http`\r\n> `/validations/dcv/dns`\r\n> `/validations/dcv/email`\r\n> ...\r\n> `/validations/caa`\r\n> `/validations/ocsp` (or other validation types)\r\n> \r\n> **Option 2:** Flat Structure with Query Parameters; simplicity, flexibility.\r\n> `/validate` (or `/verify`)\r\n> Query parameters:\r\n> `type`: `dcv`, `caa`, `ocsp`, etc.\r\n> `method`: `http`, `dns`, `email`, etc. (for DCV)\r\n> \r\n> **Option 3:** Combined Approach; balance between hierarchy and simplicity.\r\n> `/dcv`\r\n> `/dcv/http`\r\n> `/dcv/dns`\r\n> `/dcv/email`\r\n> `/caa`\r\n> `/ocsp` (or other validation types)\r\n> `/validate` for generic validation requests with query parameters \r\n\r\nPersonally I wasn't fond of any of these, although if I had to choose between the above I'd go with some version of option 1 or option 3.\r\n\r\nChatGPT 4o gave me a very different answer, seemingly being led by my prompt a bit more, so I found myself biased towards it.\r\n\r\n> 1. Endpoint Structure Overview\r\n>   **DCV** (Domain Control Validation): Since DCV involves different methods (HTTP, DNS, email, etc.), it makes sense to either have \r\n> separate endpoints for each method or a single endpoint with the method specified in the request.\r\n>   **CAA** (Certificate Authority Authorization): CAA is more straightforward, typically involving a single check to see if the domain has specific CAA records that allow the issuance of a certificate by a CA.\r\n> 2. Naming Conventions\r\n> Consistency is key. If you choose to use the term \"check\" for CAA, it might be beneficial to apply it uniformly, even to DCV, despite \"validation\" already being in the name.\r\n> 3. Potential Endpoint Structures\r\n>     1. **Option A**: Separate Endpoints for Each DCV Method\r\n>     `/dcv/http-check`: Validates domain control via HTTP.\r\n>     `/dcv/dns-check`: Validates domain control via DNS.\r\n>     `/dcv/email-check`: Validates domain control via email.\r\n>     `/caa-check`: Checks the CAA records for the domain.\r\n>     Pros: Clear separation of concerns, Easy to manage and extend if new DCV methods are added.\r\n>     Cons: Potentially more endpoints to maintain.\r\n>    \r\n>     2. **Option B**: Unified DCV Endpoint with Method in Request\r\n>     `/dcv-check`: Single endpoint for DCV validation. The method (HTTP, DNS, email) is specified in the request body or as a query parameter.\r\n>     `/caa-check`: Same as in Option A.\r\n>     Pros: Fewer endpoints, which might simplify API documentation, Easier to handle common validation logic.\r\n>     Cons: Slightly more complex request handling. Less explicit endpoint naming.\r\n\r\nI'm personally partial to ChatGPT's **Option B** (and using a request body parameter for the method), as that's the direction I've proposed taking the open-mpic reference implementation for now and it's very close to its current state (v1.0.1 has `/validation` and `/caa-check` paths). But I don't know if it's the best direction to take and figure this is one of the first things we should sort out anyway for a standard API.",
      "createdAt": "2024-08-19T06:32:54Z",
      "updatedAt": "2024-09-10T11:38:58Z",
      "closedAt": null,
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Let me add another dimension to this discussion.\r\n\r\nInspired by certificate transparency, I proposed using `/mpic/v1` (or for the draft version now `/mpic/draft00`.)\r\n\r\n- Putting `/mpic` in the prefix allows multiple services to be run under the same URL.\r\n- Putting the version in the prefix (as compared to the request) makes it easier to run two different implementations for two different versions.\r\n\r\nIn certificate transparency, all actions have a separate endpoint (`add-chain`, `add-pre-chain`, `get-entries`, `get-sth`, etc). This makes it easier to cache or rate-limit certain endpoints.\r\n\r\nI don't see a similar advantage of putting the method in the URL in case of MPIC.",
          "createdAt": "2024-08-19T09:23:06Z",
          "updatedAt": "2024-08-19T09:23:06Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks Bas, that's a good point. I think specifying `/mpic/{version}` (or draft) as the base URL is good.\r\nI also agree that putting the version in the URL is sensible if you want to enable/encourage running multiple versions of the API. Especially if you are envisioning a CT log kind of situation -- many CAs posting to a few MPIC enablers -- this makes a lot of sense. If the open source, roll-your-own-mpic implementation manages to find adoption, it may be a different dynamic, but that's far from a sure thing and this kind of flexibility in the API is valuable.\r\n\r\nThat still leaves the question of the method endpoints. I agree that `dcv-html`, `dcv-dns`, etc. being separate endpoints is not particularly useful the way it can be for CT. So it's more about what makes for a more elegant and usable spec. I personally do think that there should be _some_ request path organization of what to corroborate, so it's not just \"do mpic\" and then the requested corroboration(s) is buried in the request body. To me that would feel clunky and unconventional.\r\n\r\nHow do we feel about, for example:\r\n`/mpic/v1/caa-check` (and specifying CAA specific parameters in the body)\r\n`/mpic/v1/dcv-check` (and specifying the DCV method and other relevant parameters in the body)\r\n`/mpic/v1/dcv-with-caa-check`\r\n\r\nOr is there a preference instead for paths like like `/mpic/v1/caa` and `/mpic/v1/dcv/{method}`?",
          "createdAt": "2024-08-19T21:29:31Z",
          "updatedAt": "2024-08-19T21:29:31Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "I don't have any preference between `dcv-with-caa-check` and `dcv/caa` or `caa`.\r\n\r\nOne could move more (mandatory) arguments into the path. For instance: `/mpic/v1/caa/{domain}/{prefix}`. As every endpoint could also do CAA, we'll end up with a common endpoint internally anyway. To me it feels cleanest to also have just one endpoint in the API. It's only a slight preference.",
          "createdAt": "2024-08-21T09:18:16Z",
          "updatedAt": "2024-08-21T09:18:16Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "The main advantage of moving all mandatory parameters to the path I can think of is that the endpoint could potentially accept an empty POST body. However, I do not particularly feel this is needed and I actually think it might be detrimental. Often I feel this type of rich path is useful for resource that have GET requests associated with them since GET requests cannot have bodies, so clean encoding in the path is essential (e.g., /blog/post/{id} can accept PUT to update the post or GET to view the post). I do not think the API should use GET, so there can be a body.\r\n\r\nThe downside with mandatory parameters in the path is that it makes the API rely on two separate encoding schemes: URL and JSON. URLs have limitations on acceptable characters and non-standard characters need to be escaped. JSON also has non-acceptable characters in strings which are escaped. I would argue 1. API client implementations are more likely to get the JSON escaping correct than the URL escaping. Most languages have a JSON library which takes in native objects and just makes them compliant JSON. URL escaping is also supported in most languages, but there is a risk that implementers will just use standard string replacement to build the URLs which could cause escapement errors. 2. For most methods there is some potentially rather large mandatory data which I think is sloppy to have in a URL. Almost every method needs an expected challenge. Some Sectigo challenges I have completed use a 3-line long file. Thus, a URL with all required parts is going to read something like: /mpic/v1/dcv/http/{domain}/{path}/{challenge} where path and challenge will both have tons of escape characters as path needs to escape all the \"/\" and challenge needs to escape all the \"\\n\". Additionally, the key-value format of JSON improves readability because it is not dependent on parameter order, but instead each what each parameter is is clearly identified. IMO the readability of the above proposed URL inferior to viewing a post body that reads:\r\n{\r\ndomain: \"...\",\r\npath: \"...\",\r\nchallenge: \"...\"\r\n}\r\n\r\nFinally, I will mention that CloudFlare's original MPIC API implementation I first tested had an issue with the base64-URL escaping system used in ACME that caused an error in about 1 in 10 requests. This has since been fixed, but I do think it reinforces the point that URL escaping can be non-trivial.",
          "createdAt": "2024-08-25T05:25:02Z",
          "updatedAt": "2024-08-25T05:25:02Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "I definitely advocate not putting anything in the URL in this case as far as request-specific data, for all the reasons @birgelee lays out. Splitting parameters between the URL and body is unintuitive for clients and clunky at best for implementers. Also since this is an HTTP POST that is sending information to the server, it's expected that there is an accompanying request body.\r\n\r\nThinking about the earlier discussion points more, I also don't really like the idea of putting DCV methods (http, etc.) into the URL, because of the way the URL drives logic. DCV vs CAA is quite different logic as far how the check is carried out, and requires different configuration parameters. DCV HTTP vs DCV DNS etc. is on the other hand a bit deeper in the logic and so what will happen is the validation method will get pulled into a configuration parameter anyway, to be tossed over to the remote perspectives who then figure out what to do.\r\n\r\nI'm almost thinking Bas's idea of one endpoint for the whole thing (\"do MPIC\") and then the desired check (dcv, caa, dcv-with-caa) being a parameter might work fairly well. There's one practical issue with it, which is that I like to avoid clients needing to know magic strings whenever possible when specifying params, and this enumeration (dcv, caa, dcv-with-caa) is definitely a bit of that. Explicit URL endpoints spelled out in the OpenAPI spec make this more straightforward. The rest of the parameters are more either numbers, URLs, etc. except for DCV validation method (http, dns, etc.) which have a bit of the same issue. The other thing to worry about is, then, that we still need a parameter for the type of check we're doing, and I suppose \"check_type\" would work fine but that'd be something else to align on. I suppose in the end it's \"six of one, half-dozen of the other.\"\r\n\r\nSo I'd either go with `mpic/v1/mpic` (can't just do `mpic/v1/`) or `mpic/v1/dcv, mpic/dcv/caa` and such. Coin-toss as to which; the implementing code can be nearly identical (switch on request path vs switch on check type).",
          "createdAt": "2024-08-25T07:02:56Z",
          "updatedAt": "2024-08-25T07:03:33Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I might put in a slight preference for just one `/mpic` endpoint over `/dcv` and `/caa` because I think splitting the endpoints may cause confusion given that the `/dcv` endpoint will also do CAA by default.\r\n\r\nI was thinking and there is one advantage of using something like `/dcv/http` but it mostly exists in the OpenAPI context. In an Open API spec we can specify different JSON schemas for different endpoints allowing the distinct schema for `/dcv/http` and `/dcv/dns` to be strictly associated with the different endpoints. Thus, if I were to pass a request to `/dcv/http` with an validation details object formatted for `/dcv/dns`, we could detect this protocol violation at the spec level (there are several tools that generate request validators based on Open API specs). Given the current system (where a magic string is passed to the method parameter which determines the schema of the validation-details object), the Open API spec can only specify the validation details must be OneOf the required schemas, but cannot associate the proper schema with the magic string passed. Clearly this is an Open API specific consideration and the human-readable format of an RFC is not bound to these types of limitations.",
          "createdAt": "2024-08-26T03:37:49Z",
          "updatedAt": "2024-08-26T03:37:49Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Do we expect many clients to use the OpenAPI spec as a starting point? If so, I would not mind making the API more OpenAPI friendly. I think it's best to think about that at the end with the full API.",
          "createdAt": "2024-08-26T11:37:03Z",
          "updatedAt": "2024-08-26T11:37:03Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I don't think the open API point I brought up should be a primary concern at this stage. Lets proceed without validation methods in the url for now.",
          "createdAt": "2024-08-27T14:34:33Z",
          "updatedAt": "2024-08-27T14:34:33Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "> the /dcv endpoint will also do CAA by default\r\n\r\n@birgelee I believe the current API spec expects an explicit \"dcv-with-caa\" request for this at the moment. If `/dcv` were to do CAA by default then it would need a \"no_caa\" flag exposed. Given that, let's say we go with `/mpic` as the sole endpoint, I think we need to enumerate what validation/checking behavior can be specified.\r\n\r\n**Option A**\r\nTop-level request parameter `check_type` with values `dcv`, `caa`, and `dcv-with-caa` (current behavior options)\r\n\r\n**Option B**\r\nTop-level request parameter `check_type` with value `dcv`, `caa`, and `dcv-no-caa` (different default behavior for dcv)\r\n\r\nI don't care which option we pick, personally. They just flip the meanings of the two non-caa check types so from a client's standpoint it's identical work and so whatever is more intuitive/expected is what we ought to go with.\r\n\r\nThe name \"check_type\" is a placeholder -- we can go with a different name. In the implementation model I've for now gone with Sectigo's \"checker/check\" lexicon.",
          "createdAt": "2024-08-27T19:55:18Z",
          "updatedAt": "2024-08-27T19:55:26Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I think it makes sense to have the `/mpic` endpoint. @bwesterb Didn't you mention you like the CAA check with DCV by default idea?",
          "createdAt": "2024-08-27T20:06:54Z",
          "updatedAt": "2024-08-27T20:06:54Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "In the current text, there is a `method` field in the request object which is either `http`, `dns`, or `caa`. The CAA check is also preformed for the `http` and `dns` methods unless the optional `caa-check` field is set to false. This seems very similar to @sciros' proposal. ",
          "createdAt": "2024-08-29T11:23:37Z",
          "updatedAt": "2024-08-29T11:23:37Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "If one API endpoint is used I think it should accept a list of checks to be performed together, rather than having to make multiple requests for each validation type. This also allows new semantics to slot in neatly if there's some new CAA like thing that CAs want to check along with HTTP/DNS and CAA.\r\n\r\nI support explicit protocol versioning. I think there should be a metadata document at `/mpic/directory` (or maybe even `/.well-known/mpic` given this is IETF work?) that lists what versions the server supports, and configuration parameters (not that I can currently see a need for those). This would allow a client to perform version negotiation, simplifying moving between MPIC providers. ",
          "createdAt": "2024-09-06T11:58:09Z",
          "updatedAt": "2024-09-06T11:58:09Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "> I think there should be a metadata document [...]\r\n\r\nIf required we can punt that to when we actually have a second version. A missing directory (or whatever the mechanism would be) indicates v1.\r\n\r\n(Context: I am trying to keep the initial draft as simple as possible, but no simpler.)",
          "createdAt": "2024-09-06T12:06:07Z",
          "updatedAt": "2024-09-06T12:06:07Z"
        },
        {
          "author": "romanf",
          "authorAssociation": "NONE",
          "body": "Not sure if this question belongs here: Has it been decided that one endpoint will do the corroboration from multiple perspectives? So if somebody wants to run an MPIC instance that party would have to have 2-6 remotes? Or is it the intention that one endpoint will be one perspective?",
          "createdAt": "2024-09-06T12:09:01Z",
          "updatedAt": "2024-09-06T12:09:01Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "> Not sure if this question belongs here: Has it been decided that one endpoint will do the corroboration from multiple perspectives? So if somebody wants to run an MPIC instance that party would have to have 2-6 remotes? Or is it the intention that one endpoint will be one perspective?\r\n\r\nThe single endpoint will do corroboration from multiple perspectives.",
          "createdAt": "2024-09-06T12:19:08Z",
          "updatedAt": "2024-09-06T12:19:08Z"
        },
        {
          "author": "romanf",
          "authorAssociation": "NONE",
          "body": "Wouldn't it be more flexible to use a model like with Certificate Transparency? Where a number of organizations could each run one perspective and CAs select the required number of perspectives?",
          "createdAt": "2024-09-06T12:35:17Z",
          "updatedAt": "2024-09-06T12:35:17Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Let's discuss that in #15 ",
          "createdAt": "2024-09-06T12:39:17Z",
          "updatedAt": "2024-09-06T12:39:17Z"
        }
      ]
    },
    {
      "number": 9,
      "id": "I_kwDOMYks0s6Tn0qS",
      "title": "Validation response structure",
      "url": "https://github.com/open-mpic/draft-mpic/issues/9",
      "state": "OPEN",
      "author": "SulemanAhmadd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "Should the API provide the response from each vantage point / perspective back to the client? Or should it provide a single overall result without exposing the results of individual perspectives?",
      "createdAt": "2024-08-20T23:17:08Z",
      "updatedAt": "2024-08-26T11:53:43Z",
      "closedAt": null,
      "comments": [
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "Good question. My thinking is the API should absolutely, at least by default, provide the response from each perspective as well as an overall result with transparency as to the quorum count and number of perspectives included in the interrogated cohort, as I would not want to assume that each perspective is always \"healthy\"/reliable and should always be included in a cohort of perspectives to interrogate going forward. Not only that, but I think we are going to be in a \"learning\" phase for a time with MPIC and having more transparency into what happens will only help as far as iterating towards the most reliable way of doing it.\r\n\r\nIt may be worth exploring the idea of an optional \"verbose\" vs \"non-verbose\" response, in case the client is sometimes interested in nothing beyond a yes/no answer.\r\n\r\nI also think this may be a compliance question -- does the _issuing party_ have an obligation to meet the logging requirements associated with MPIC? Because those are fairly exhaustive as far as what gets recorded.",
          "createdAt": "2024-08-21T02:56:31Z",
          "updatedAt": "2024-08-21T02:56:31Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "There are a few downsides to making this more verbose. First, it's harder to implement for the server and the client. Also, it might suggest that the client should implement the quorum logic themselves (if we return the responses on both success and failure).\r\n\r\nI agree that we need good insight initially. The target audience for this API are smaller CAs. Are they the ones that will do the learning? We can expose data in different ways.\r\n\r\nThe logging requirements are a convincing point \u2014 do you have a pointer for them?",
          "createdAt": "2024-08-21T09:13:56Z",
          "updatedAt": "2024-08-21T09:13:56Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I was involved in drafting the logging requirements for the CAB forum. The current requirements read that a CA must log the following:\r\n\r\nMulti-Perspective Issuance Corroboration attempts from each Network Perspective, minimally recording the following information:\r\n\r\n    a. an identifier that uniquely identifies the Network Perspective used;\r\n    b. the attempted domain name and/or IP address; and\r\n    c. the result of the attempt (e.g., \"domain validation pass/fail\", \"CAA permission/prohibition\").\r\n\r\nMulti-Perspective Issuance Corroboration quorum results for each attempted domain name or IP address represented in a Certificate request (i.e., \"3/4\" which should be interpreted as \"Three (3) out of four (4) attempted Network Perspectives corroborated the determinations made by the Primary Network Perspective).\r\n\r\nI advocate that the response from the API contain all of the data required to satisfy the logging requirements. I think this is particularly the case if we want to sign responses. Having the API include the logging requirements makes implementation easy: a CA simply needs to pipe API responses to its existing certificate issuance log stream. If the API response does not have this information, the CA has to do additional work to ensure it has access to the log stream from the API host and I personally think that is a bit of a mess (Open MPIC right now does not even use cloud watch logs which could get very large and costly if we actually had to log all of these requests in AWS). I think there are ways to discourage CAs from parsing out the corroboration data and coming to their own quorum conclusions.",
          "createdAt": "2024-08-21T16:37:05Z",
          "updatedAt": "2024-08-21T16:37:05Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Ok, given those requirements we have to add support for it to the API. What about having a single `log` field in the response that is a string that contains the necessary information, but we leave the exact format unspecified so that clients are discouraged from parsing it?",
          "createdAt": "2024-08-23T12:09:40Z",
          "updatedAt": "2024-08-23T12:09:40Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I am fine with that. I like the log field idea.",
          "createdAt": "2024-08-25T05:25:48Z",
          "updatedAt": "2024-08-25T05:25:48Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "So would the API response be something like \r\n```{\r\n    statusCode: 200,\r\n    body: {\r\n        is_valid: true,\r\n        log: \"json-serialized-object-with-relevant-info-here\"\r\n    }\r\n}\r\n```\r\nI suppose that's OK, although just barely.\r\n\r\nFor what it's worth, I don't see a compelling argument for discouraging clients from parsing and understanding the details of the response. I suppose not wanting to define and therefore document the structure of the response object because of implementation flexibility, that's one thing. But what information the response object contains, that pretty much _has_ to be part of the contract. CAs will have to prove they're logging what they are required to log, at the very least, so I would _expect_ that info to be validated client-side, at least initially, to ensure it's sufficient to meet logging requirements. And it'll need to be verifiable at any time in any case.",
          "createdAt": "2024-08-25T07:37:55Z",
          "updatedAt": "2024-08-25T07:38:47Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "actually thinking more I see @sciros 's point. Logging is something CAs are audited on. For any CA to use an MPIC API, they will have to show the API is compliant with the logging requirements. Thus, every implementation will have to publish what they are going to put into the log field and how it will be formatted. Given that we have the opportunity to standardize this across deployments, I think we should put that in.\r\n\r\nIn several other conversations we have moved the spec in a way to ensure that an API call to a MPIC RFC implementation is as likely to be CA/F Forum compliant as possible. Here we are dropping an opportunity to strictly specify what must be logged to ensure we meet the logging requirements.\r\n\r\n@sciros also has a good point that, at least for Open MPIC, this field will simply be something like `json.stringify(perspective_object)` making it just JSON in JSON. Simply imparting a logging structure would be helpful as it could just make this an object. I also do not think we can stop a CA that wants to go out of their way to enforce a non-compliant quorum policy as opposed to simply using the is_valid field. ",
          "createdAt": "2024-08-26T03:25:59Z",
          "updatedAt": "2024-08-26T03:25:59Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "The primary upside of a more unstructured field, is that it requires less standardisation work.\r\n\r\nI just realised there is a little snag with either approach if we expect the log (structured or not) to be used as full proof. The quoted third requirement is:\r\n\r\n> the result of the attempt (e.g., \"domain validation pass/fail\", \"CAA permission/prohibition\").\r\n\r\nWith the currently proposed API the MPIC service does not always know if CAA is permitted or fails, or whether domain validation passes or fails. The API call would need much more context about how the validation works precisely, and the context of the CA.",
          "createdAt": "2024-08-26T11:30:22Z",
          "updatedAt": "2024-08-26T11:30:22Z"
        },
        {
          "author": "gcimaszewski",
          "authorAssociation": "COLLABORATOR",
          "body": "I would personally lean towards providing more details of the response from each vantage point, as this information is useful for debugging and testing purposes. Merely returning \"pass/fail\" is very opaque, and in early stages of deployment, it's hard to tell whether a failed validation is due to the challenge/CAA check actually failing or some misconfiguration in cloud infrastructure. (This is especially true for the DNS case, as a timeout or SERVFAIL is very different from retrieving a record with contents different from what was expected.)\r\nPerhaps these additional details could also be specified as an object field, so we don't need to hammer out the exact fields that will be provided?",
          "createdAt": "2024-08-26T11:53:43Z",
          "updatedAt": "2024-08-26T11:53:43Z"
        }
      ]
    },
    {
      "number": 10,
      "id": "I_kwDOMYks0s6TrD2c",
      "title": "Should we always perform a CAA check on behalf of the client?",
      "url": "https://github.com/open-mpic/draft-mpic/issues/10",
      "state": "CLOSED",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "(Resolved, but writing down for posterity.)\r\n\r\nThe MPIC service is in a position to enforce CAA for the DCV validation of the client, given it knows the issuerid of the requesting CA. Should the MPIC service enforce CAA (as in the Cloudflare API) or leave it to the client?\r\n\r\n@birgelee and @sciros point out that the answer must be no. CAs are allowed to defer DCV for subdomain to the domain itself. The CA still has to check the CAA records of the subdomain. This flow could be broken if the MPIC service would check CAA on the top-level domain.",
      "createdAt": "2024-08-21T09:25:22Z",
      "updatedAt": "2024-08-21T09:25:28Z",
      "closedAt": "2024-08-21T09:25:28Z",
      "comments": []
    },
    {
      "number": 11,
      "id": "I_kwDOMYks0s6TrE0h",
      "title": "Client authentiation",
      "url": "https://github.com/open-mpic/draft-mpic/issues/11",
      "state": "CLOSED",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [
        "duplicate"
      ],
      "body": "Do we want to suggest the preferred method of client authentication (eg. HTTP bearer token / mTLS) or leave it to the specific service?",
      "createdAt": "2024-08-21T09:27:19Z",
      "updatedAt": "2024-08-21T15:54:22Z",
      "closedAt": "2024-08-21T15:54:17Z",
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Duplicate of #2.",
          "createdAt": "2024-08-21T15:54:17Z",
          "updatedAt": "2024-08-21T15:54:17Z"
        }
      ]
    },
    {
      "number": 12,
      "id": "I_kwDOMYks0s6Tr9xg",
      "title": "Prepare slides for SECDISPATCH",
      "url": "https://github.com/open-mpic/draft-mpic/issues/12",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "",
      "createdAt": "2024-08-21T11:20:02Z",
      "updatedAt": "2024-08-21T11:20:02Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 13,
      "id": "I_kwDOMYks0s6UKlfP",
      "title": "CAA encoding",
      "url": "https://github.com/open-mpic/draft-mpic/issues/13",
      "state": "OPEN",
      "author": "birgelee",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "https://github.com/bwesterb/draft-mpic/blob/67f9369432bb332c4ae8cc96da1912a212ca222f/draft-westerbaan-secdispatch-mpic.md?plain=1#L141\r\n\r\nI noticed CloudFlare uses base64 representation of CAA records in their internal testing. Is there reason this is preferable to the Canonical Presentation Format for CAA expressed [RFC 6844 Sec 5.1.1.](https://datatracker.ietf.org/doc/html/rfc6844#section-5.1.1) which is more human readable?",
      "createdAt": "2024-08-26T03:44:52Z",
      "updatedAt": "2024-09-06T20:49:20Z",
      "closedAt": null,
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "From that same RFC:\r\n\r\n> Value:    A sequence of octets representing the property value.\r\n>      Property values are encoded as binary values and MAY employ sub-\r\n>      formats.\r\n\r\nSo we might see a future new tag, that has a different preferred human readable format than value as character-string (defined in 5.1.1).\r\n\r\nAlso, parsing a character-string is a bit more of a hassle and easy to get wrong:\r\n\r\n- How to interpret `CAA 0 issue @`?\r\n- Will the client `CAA 0 issue (lets\\nencrypt)` correctly?\r\n- What is the meaning of a semicolon? `CAA 0 issue letsencrypt.com;isthisacomment ; or this?`\r\n- This is allowed: `CAA 0 issue \\l\\e\\t\\s\\e\\n\\c\\r\\121pt.com`\r\n",
          "createdAt": "2024-08-26T10:41:52Z",
          "updatedAt": "2024-08-26T10:57:48Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "I agree Base64 is probably better here to avoid DNS text parsing pitfalls.\r\n\r\nThe draft needs to be more specific on what exactly goes into the field. Is it the whole RR ([RFC 1035 \u00a7 3.2.1](https://datatracker.ietf.org/doc/html/rfc1035#section-3.2.1)), whole RRsets ([RFC 1035 \u00a7 4.1](https://datatracker.ietf.org/doc/html/rfc1035#section-4.1)), just the RDATA encoding ([RFC 6844 \u00a7 5.1](https://datatracker.ietf.org/doc/html/rfc6844#section-5.1)), or something else?\r\n\r\nDrawing on points from [my comment in #6](https://github.com/open-mpic/draft-mpic/issues/6#issuecomment-2333871183) I think the DNSSEC chain (or lack there of) should also be returned. This provides some non-reputability of the CAA records, as well as allowing a CA to see if some vantage points see DNSSEC signatures where others don't - possibly a sign of meddling. ",
          "createdAt": "2024-09-06T11:48:29Z",
          "updatedAt": "2024-09-06T11:48:29Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "@TheEnbyperor I think the idea of returning the DNSSEC chain is interesting. Do recommend any production DNS software that can produce these chains and also perform validation of the chains offline?\r\n\r\nThere also may be an added complexity in the Open MPIC case given that we rely on AWS's default DNS resolver. I believe it is DNSSEC validating, but we only get the AD bit back, not the entire chain.",
          "createdAt": "2024-09-06T16:57:45Z",
          "updatedAt": "2024-09-06T16:57:45Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "I'm not aware of any DNS software daemon that will return the entire chain, but I'm quite familiar with the hickory DNS package for Rust that I'm pretty sure will let you collect all that information - although I haven't checked the docs thoroughly for that. https://github.com/hickory-dns/hickory-dns",
          "createdAt": "2024-09-06T17:37:13Z",
          "updatedAt": "2024-09-06T17:37:13Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "Thanks @TheEnbyperor . Even though the resolver collects the information, I do not think there is any current support for packaging the DNSSEC chain info in a format for later consumption. Hickory DNS is also not fully production ready yet with some things like NSEC3 validation not being implemented.\r\n\r\nWe also work with Josh Aas who manages Hickory development, this may be something he would be interested in.\r\n\r\nWhile I am interested in DNSSEC chain proofs, I hesitate a bit to put it into the standard without there being a substantial deployment ecosystem around the generation and consumption of such proofs.",
          "createdAt": "2024-09-06T19:32:14Z",
          "updatedAt": "2024-09-06T19:32:14Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "I agree on maintaining Base64 encoding to make parsing of the records easier. I believe we would want to return each applicable RR from the set in the `records` field (in the current design). Its currently implied but we can make it explicit.\r\n\r\nResolvers validate the DNSSEC chains themselves and there is no current mechanism to return a \"proof\" to the client. Returning entire chains can also exceed practical limits of UDP message sizes 64K and can be a source of major amplification attack problem. Lets discuss this suggestion under a separate issue: https://github.com/open-mpic/draft-mpic/issues/16",
          "createdAt": "2024-09-06T20:49:19Z",
          "updatedAt": "2024-09-06T20:49:19Z"
        }
      ]
    },
    {
      "number": 14,
      "id": "I_kwDOMYks0s6UN5SF",
      "title": "Fix build",
      "url": "https://github.com/open-mpic/draft-mpic/issues/14",
      "state": "CLOSED",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [
        "birgelee"
      ],
      "labels": [],
      "body": "@birgelee could you set \"Workflow permissions\" to \"Read and write permissions\" [in the repository settings](https://github.com/open-mpic/draft-mpic/settings/actions#actions_default_workflow_permissions_write)? That probably fixed the build. I lost admin rights after transferring #1 ",
      "createdAt": "2024-08-26T11:50:26Z",
      "updatedAt": "2024-08-29T10:58:42Z",
      "closedAt": "2024-08-29T10:58:42Z",
      "comments": [
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I had to change this at the org level, but it should be updated now. Please let me know if you the run is still having issues.",
          "createdAt": "2024-08-26T16:59:49Z",
          "updatedAt": "2024-08-26T16:59:49Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "That fixed it. Thanks.",
          "createdAt": "2024-08-29T10:58:42Z",
          "updatedAt": "2024-08-29T10:58:42Z"
        }
      ]
    },
    {
      "number": 15,
      "id": "I_kwDOMYks0s6VoMQs",
      "title": "API for single perspectives",
      "url": "https://github.com/open-mpic/draft-mpic/issues/15",
      "state": "OPEN",
      "author": "bwesterb",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "> Wouldn't it be more flexible to use a model like with Certificate Transparency? Where a number of organizations could each run one perspective and CAs select the required number of perspectives?\r\n\r\n_Originally posted by @romanf in https://github.com/open-mpic/draft-mpic/issues/8#issuecomment-2333957238_\r\n            ",
      "createdAt": "2024-09-06T12:38:29Z",
      "updatedAt": "2024-09-06T22:46:11Z",
      "closedAt": null,
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "This is an interesting suggestion. This is more difficult to use for a client than the current MPIC API, but has the potential to have a broader ecosystem.\r\n\r\nThe crux is: are there parties that are not willing to run (and open up) a global multi-perspective network, but are willing to run single perspectives. And are these parties sufficiently globally distributed? (Crucially, we want perspectives outside of Europe and North America.)",
          "createdAt": "2024-09-06T12:41:59Z",
          "updatedAt": "2024-09-06T14:55:57Z"
        },
        {
          "author": "romanf",
          "authorAssociation": "NONE",
          "body": "I can answer for our org... we would be willing to run one perspective (probably redundant from our datacenters which are both located in Switzerland)... since that would basically not cause any additional cost.\r\nRunning a multi-perspective network would not cuase extra cost for big organisations (e.g. Google, Amazon, Microsoft, Cloudflare....) that have the gloal network already. But I'm not sure if they would be willing to offer their perspectives to CAs for free... ;)",
          "createdAt": "2024-09-06T12:49:10Z",
          "updatedAt": "2024-09-06T12:49:10Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "I can't speak for the other large providers, but Cloudflare [has committed](https://blog.cloudflare.com/secure-certificate-issuance/) to offer the service for free.",
          "createdAt": "2024-09-06T13:06:20Z",
          "updatedAt": "2024-09-06T13:06:20Z"
        },
        {
          "author": "TheEnbyperor",
          "authorAssociation": "NONE",
          "body": "Speaking with my Glauca Digital hat on we'd also be willing to offer MPIC endpoints for one or two datacenters we have in Europe, but it wouldn't be feasible for us to offer a global service.",
          "createdAt": "2024-09-06T14:39:35Z",
          "updatedAt": "2024-09-06T14:39:35Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I am happy this question came up in discussion. This actually reminds me of something Ryan Hurst (https://www.linkedin.com/in/ryanmhurst) had discussed with me where CAs assist other CAs with DCV.\r\n\r\nIn general I like the idea of moving in this direction, but it would likely require a different standard in addition to this proposed draft. I think this draft should be kept explicitly for the interface between CA infrastructure and an MPIC service. I feel standardizing the communication between an MPIC service and multiple perspectives run by different 3rd-party organizations is a good idea but deserves a separate standards draft.\r\n\r\nEven prior to standardization, we may be able to offer something along these lines in the Open MPIC implementation.",
          "createdAt": "2024-09-06T15:56:59Z",
          "updatedAt": "2024-09-06T15:56:59Z"
        },
        {
          "author": "sciros",
          "authorAssociation": "COLLABORATOR",
          "body": "Yes, from my point of view, this is a different paradigm from the single-endpoint MPIC approach and would require different endpoint definition(s).\r\n\r\nThe current open-mpic specification describes a classic client-server topology, i.e. a service which abstracts away the entire orchestration of MPIC, including how remote perspectives are interrogated and how quorum is determined. The endpoint of interest is client-to-orchestrator. (The orchestrator-to-remote-perspective integration is in this case not specified.)\r\n\r\nThe peer-to-peer model @romanf describes would have each peer carry our their own orchestration, so the endpoint specifications involved would be orchestrator-to-remote-perspective, rather than client-to-orchestrator.\r\n\r\nBoth models are valid per se, and presumably for some CAs one is a better fit than the other. However I believe the assumptions that underlie each, and the practical challenges inherent in each, are materially distinct enough that while one solution can _inform_ the other, I don't think one renders the other obsolete nor can one subsume the other.",
          "createdAt": "2024-09-06T20:37:33Z",
          "updatedAt": "2024-09-06T20:37:33Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "Note that there are two different approaches: 1) CA selects its preference of single perspectives to use (changes the current API completely), or 2) The MPIC service (orchestrator) does it for the client transparently, similar to what we are doing right now but limited to multiple perspectives under the same org.\r\n\r\nThe former is obviously harder for the client. For the latter, yes, we have not defined any perspective <-> perspective API interface in the draft, but should we consider that strictly out of scope? Surely something to explain in the operations section of the draft. If we do, whether each of those perspectives belongs to the same organization or different organizations, the API interface can be made to remain consistent. It means more state management for the orchestrating MPIC service for sure i.e. if availability of compliant perspectives under org are less than quorum then reach out to other available perspectives from other orgs (easy on paper but can be difficult to coordinate the selection mechanism).\r\n\r\nRegardless, one of the biggest challenge with the single perspective model is determining whether selection of third-party perspectives are sufficiently distributed to meet the distance requirements. 1, 2 or multiple perspectives, the problem here can be generalized to cross organization MPIC.",
          "createdAt": "2024-09-06T22:45:29Z",
          "updatedAt": "2024-09-06T22:46:11Z"
        }
      ]
    },
    {
      "number": 16,
      "id": "I_kwDOMYks0s6VrXEz",
      "title": "DNSSEC validation response for CAA checks from each perspective",
      "url": "https://github.com/open-mpic/draft-mpic/issues/16",
      "state": "OPEN",
      "author": "SulemanAhmadd",
      "authorAssociation": "COLLABORATOR",
      "assignees": [],
      "labels": [],
      "body": "> I think the DNSSEC chain (or lack there of) should also be returned. This provides some non-reputability of the CAA records, as well as allowing a CA to see if some vantage points see DNSSEC signatures where others don't - possibly a sign of meddling.\r\n\r\nWas brought up by @TheEnbyperor. Although an interesting idea for better transparency and non-repudiation, in the current state of the world, DNS resolvers do not return DNSSEC authentication chains back to the clients (for practical reasons). The validation is done by the resolver, and the client _trusts_ the process.\r\n\r\nWe can recommend in the draft that for CAA checks, perspectives should prefer use of DNSSEC validating resolvers. By setting the DO bit, we can receive the RRSIG record for CAA of the domain, and the AD bit gives indication that the resolver performed the validation.\r\n\r\nThe question then boils down to, should each perspective return the base64 encoded RRSIG record as well in the response for the CAA check? Would that help provide value for non-reputability?",
      "createdAt": "2024-09-06T20:47:03Z",
      "updatedAt": "2024-09-16T10:52:53Z",
      "closedAt": null,
      "comments": [
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I do not think having the RRSIG is useful because without the full chain the signature cannot be verified. I would advise recording the AD bit and recommending DNSSEC and keeping it at that.",
          "createdAt": "2024-09-07T21:14:25Z",
          "updatedAt": "2024-09-07T21:14:25Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "We should take a step back and first consider the question: is there any benefit to check DNSSEC *from multiple perspectives*. It is clear that checking DNSSEC has a clear benefit from a single perspective, but does that benefit increase by checking it from multiple perspectives? I would say it does not.\r\n\r\nNonetheless it's good to mandate checking DNSSEC from each perspective as it's cheap.\r\n\r\nHowever, serialising and returning the full chain requires a significant amount of work in standardisation and implementation.\r\n\r\nThere is value in a CA having to log the DNSSEC chain from its own validation, but to me that is out of scope for an initial draft. We could well add it to MPIC at a later stage as a convenience, but it should not block progress.",
          "createdAt": "2024-09-11T12:07:03Z",
          "updatedAt": "2024-09-11T12:07:03Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "For context, here is [CA/B discussion](https://github.com/cabforum/servercert/issues/459) on adding validation method to certificates.",
          "createdAt": "2024-09-16T10:52:51Z",
          "updatedAt": "2024-09-16T10:52:51Z"
        }
      ]
    },
    {
      "number": 17,
      "id": "I_kwDOMYks0s6WyEtb",
      "title": "Domain or IP",
      "url": "https://github.com/open-mpic/draft-mpic/issues/17",
      "state": "OPEN",
      "author": "birgelee",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "https://github.com/open-mpic/draft-mpic/blob/42ee5bf17254cf81efa879cfee2bd1b51fa8c718/draft-westerbaan-secdispatch-mpic.md?plain=1#L110\r\n\r\nThe draft currently uses the label `domain` to refer to the domain being validated. ACME and the BRs also support certs for IP addresses. Open MPIC currently refers to this as `domain_or_ip_target`.\r\n\r\nDo we want to change the language to allow this identifier to be an IP address or a domain? How should IP address targets be handled in the future?",
      "createdAt": "2024-09-17T00:14:07Z",
      "updatedAt": "2024-09-17T21:34:23Z",
      "closedAt": null,
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "How is a CAA check performed for an IP address?",
          "createdAt": "2024-09-17T12:53:28Z",
          "updatedAt": "2024-09-17T12:53:39Z"
        },
        {
          "author": "gcimaszewski",
          "authorAssociation": "COLLABORATOR",
          "body": "@bwesterb In the BRs, it looks like CAA checking only applies to fully-qualified domain names, and thus there is no CAA check for IPs. \r\nThe API would also need some more changes to support IP validation, like a PTR lookup for DNS-based validation. ",
          "createdAt": "2024-09-17T19:04:39Z",
          "updatedAt": "2024-09-17T19:04:39Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "The [BR document](https://github.com/cabforum/servercert/blob/main/docs/BR.md) states: `As part of the Certificate issuance process, the CA MUST retrieve and process CAA records in accordance with RFC 8659 for each **dNSName** in the subjectAltName extension`\r\n\r\nSo it seems validation of only domain name entries on the SAN is required when validated CAA records. Not sure if CAA records for IP addresses are a thing. There is an [expired draft in LAMPS working group](https://www.ietf.org/archive/id/draft-chariton-ipcaa-00.html) at IETF for this line of work but it seems to be abandoned.",
          "createdAt": "2024-09-17T21:26:25Z",
          "updatedAt": "2024-09-17T21:27:45Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "Ah, I was on a stale page so @gcimaszewski comment just got visible. With that context, I believe this issue is about supporting IP address control validation on the API, IIUC. The first comment on this issue references the CAA check description, hence the confusion.\r\n\r\nThe HTTP validation method in the draft can be generalized for both domain and IP address in that case.",
          "createdAt": "2024-09-17T21:33:51Z",
          "updatedAt": "2024-09-17T21:34:23Z"
        }
      ]
    },
    {
      "number": 18,
      "id": "I_kwDOMYks0s6WyFC2",
      "title": "Perspective response",
      "url": "https://github.com/open-mpic/draft-mpic/issues/18",
      "state": "OPEN",
      "author": "birgelee",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "https://github.com/open-mpic/draft-mpic/blob/42ee5bf17254cf81efa879cfee2bd1b51fa8c718/draft-westerbaan-secdispatch-mpic.md?plain=1#L141\r\n\r\nGiven the discussion in the logging issue, I think putting a response from each perspective in the API response is essential. Additionally, documentation on the perspectives used and their corresponding geographic regions would be good.",
      "createdAt": "2024-09-17T00:15:49Z",
      "updatedAt": "2024-09-17T00:15:50Z",
      "closedAt": null,
      "comments": []
    },
    {
      "number": 19,
      "id": "I_kwDOMYks0s6WyFgn",
      "title": "ACME-specific methods",
      "url": "https://github.com/open-mpic/draft-mpic/issues/19",
      "state": "OPEN",
      "author": "birgelee",
      "authorAssociation": "MEMBER",
      "assignees": [],
      "labels": [],
      "body": "https://github.com/open-mpic/draft-mpic/blob/42ee5bf17254cf81efa879cfee2bd1b51fa8c718/draft-westerbaan-secdispatch-mpic.md?plain=1#L187\r\n\r\nUpon reviewing the BRs and the ACME RFC, I plan to rework the open mpic API to separate agreed website change v2 and ACME agreed website change as separate methods. I know this came up previously and I sided on keeping them as a single method, but I have changed my mind a bit upon more review of the RFC and BRs. I will put a more detailed discussion in the Open MPIC repo when I make a PR for the change. Just a heads up.",
      "createdAt": "2024-09-17T00:18:12Z",
      "updatedAt": "2024-09-18T09:43:19Z",
      "closedAt": null,
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "What is the reasoning for the change of heart?",
          "createdAt": "2024-09-17T12:49:55Z",
          "updatedAt": "2024-09-17T12:49:55Z"
        },
        {
          "author": "birgelee",
          "authorAssociation": "MEMBER",
          "body": "I put in some discussion in the aws-paython-lambda repo:\r\nhttps://github.com/open-mpic/aws-lambda-python/issues/13#issue-2531872132\r\n\r\nEssentially the standard seems to clearly identify these as two separate methods that actually have fairly significant differences in how to properly execute validation. It is not clear to me upon further review of the standards that a single back end engine could appropriately handle both methods without it simply being two methods under the hood masquerading as a single method. I know this is somewhat contrary to recommendations I made earlier but I had not realized the extent to which these two methods contradicted.",
          "createdAt": "2024-09-17T19:03:44Z",
          "updatedAt": "2024-09-17T19:03:44Z"
        },
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Maybe I'm missing something, but we could still use a single method if the API returns the quorum value of the file, and only accesses on HTTP?",
          "createdAt": "2024-09-18T09:43:18Z",
          "updatedAt": "2024-09-18T09:43:18Z"
        }
      ]
    }
  ],
  "pulls": [
    {
      "number": 20,
      "id": "PR_kwDOMYks0s58sYOd",
      "title": "Update abstract",
      "url": "https://github.com/open-mpic/draft-mpic/pull/20",
      "state": "MERGED",
      "author": "ryancdickson",
      "authorAssociation": "CONTRIBUTOR",
      "assignees": [],
      "labels": [],
      "body": "Sorry for the slow start. Will spend time reviewing the existing content over the next few days, and will suggest updates via PR.\r\n\r\nNo hard feelings if suggestions are not accepted!",
      "createdAt": "2024-09-25T18:02:32Z",
      "updatedAt": "2024-09-25T21:39:44Z",
      "baseRepository": "open-mpic/draft-mpic",
      "baseRefName": "main",
      "baseRefOid": "815afb0ce7d6629b70420a6f93e5caf7541ae38d",
      "headRepository": "ryancdickson/draft-mpic",
      "headRefName": "main",
      "headRefOid": "a2a0c4db39a6d918703e933f066dd9522c73cbc7",
      "closedAt": "2024-09-25T21:39:43Z",
      "mergedAt": "2024-09-25T21:39:43Z",
      "mergedBy": "bwesterb",
      "mergeCommit": {
        "oid": "5e253503848b11e0249bf03833134b588bff078f"
      },
      "comments": [
        {
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "body": "Thanks @ryancdickson.  I think this is an improvement for readers who want to understand the purpose that only read the abstract. @SulemanAhmadd to move things along, I propose we just merge and refine in later PRs if you want it terser.",
          "createdAt": "2024-09-25T20:10:11Z",
          "updatedAt": "2024-09-25T20:10:34Z"
        },
        {
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "body": "Happy to cater to them later. Suggestions are non-blockers!",
          "createdAt": "2024-09-25T20:19:35Z",
          "updatedAt": "2024-09-25T20:19:35Z"
        }
      ],
      "reviews": [
        {
          "id": "PRR_kwDOMYks0s6K1HU9",
          "commit": {
            "abbreviatedOid": "a2a0c4d"
          },
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "state": "COMMENTED",
          "body": "Some minor suggestions to consider. Thanks for the PR! :)",
          "createdAt": "2024-09-25T18:29:08Z",
          "updatedAt": "2024-09-25T18:40:07Z",
          "comments": [
            {
              "originalPosition": 8,
              "body": "`MPIC enhances the security of publicly-trusted certificate issuance by mitigating the risk of localized, equally-specific BGP hijacking attacks that can undermine traditional DCV methods permitted by the CA/Browser Forum Baseline Requirements for TLS Server Certificates.`\r\n\r\nIn order to keep abstract precise and to the point, may be we should move this line to the introduction below instead?",
              "createdAt": "2024-09-25T18:29:08Z",
              "updatedAt": "2024-09-25T18:40:07Z"
            },
            {
              "originalPosition": 8,
              "body": "Consider dropping: `The API design prioritizes flexibility, scalability, and interoperability, allowing for diverse implementations and deployment models.`\r\n\r\nAgain, to keep abstract to the point.",
              "createdAt": "2024-09-25T18:31:31Z",
              "updatedAt": "2024-09-25T18:40:07Z"
            },
            {
              "originalPosition": 8,
              "body": "`This standardization effort is driven by the need...`, instead of that lets be more direct as this document relates to standardizing the API.\r\n\r\n`This document defines the API to perform MPIC as reflected in Ballot SC-067 V3...`",
              "createdAt": "2024-09-25T18:37:34Z",
              "updatedAt": "2024-09-25T18:40:07Z"
            }
          ]
        },
        {
          "id": "PRR_kwDOMYks0s6K2F1N",
          "commit": {
            "abbreviatedOid": "a2a0c4d"
          },
          "author": "bwesterb",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-09-25T20:07:43Z",
          "updatedAt": "2024-09-25T20:07:43Z",
          "comments": []
        },
        {
          "id": "PRR_kwDOMYks0s6K2P_J",
          "commit": {
            "abbreviatedOid": "a2a0c4d"
          },
          "author": "SulemanAhmadd",
          "authorAssociation": "COLLABORATOR",
          "state": "APPROVED",
          "body": "",
          "createdAt": "2024-09-25T20:31:23Z",
          "updatedAt": "2024-09-25T20:31:23Z",
          "comments": []
        }
      ]
    }
  ]
}